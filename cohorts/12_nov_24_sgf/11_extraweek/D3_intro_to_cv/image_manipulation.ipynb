{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Get images with DuckDuckGo](#toc1_1_)    \n",
    "- [Read images with OpenCV](#toc2_)    \n",
    "  - [B&W images](#toc2_1_)    \n",
    "  - [Color images](#toc2_2_)    \n",
    "    - [Split into BGR, HSV](#toc2_2_1_)    \n",
    "- [Image Manipulations](#toc3_)    \n",
    "  - [Cropping](#toc3_1_)    \n",
    "  - [Resizing](#toc3_2_)    \n",
    "    - [To a specific dimension](#toc3_2_1_)    \n",
    "    - [While maintaining aspect ratio](#toc3_2_2_)    \n",
    "  - [Flipping](#toc3_3_)    \n",
    "  - [Brightness control - Addition](#toc3_4_)    \n",
    "  - [Contrast control - multiplication](#toc3_5_)    \n",
    "  - [Thresholding](#toc3_6_)    \n",
    "    - [Application: OCR](#toc3_6_1_)    \n",
    "- [Demo - live image manipulation](#toc4_)    \n",
    "- [References](#toc5_)    \n",
    "- [Extra](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "from utils import search_images_ddg, download_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Get images with DuckDuckGo](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_key = 'misaligned forms'\n",
    "url_set = search_images_ddg(search_key, max_n=25)\n",
    "download_images(image_urls=url_set, image_prefix=search_key.replace(\" \", \"_\"), output_folder=f\"img/image-manipulation/form_pictures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Read images with OpenCV](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[B&W images](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"img/image-manipulation/checkered_board/checkered_board_1.jpg\"\n",
    "\n",
    "# We can display the image directly in the Jupyter notebook\n",
    "Image(filename=img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image as gray scale.\n",
    "cb_img = cv2.imread(img_path, 0)\n",
    "print(cb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the size  of image\n",
    "print(\"Image size (H, W) is:\", cb_img.shape)\n",
    "\n",
    "# print data-type of image\n",
    "print(\"Data type of image is:\", cb_img.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Color images](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in image\n",
    "img_path = \"img/image-manipulation/landscape/landscape_1.jpg\"\n",
    "\n",
    "img = cv2.imread(img_path, 1)\n",
    "\n",
    "# print the size  of image\n",
    "print(\"Image size (H, W, C) is:\", img.shape)\n",
    "\n",
    "# print data-type of image\n",
    "print(\"Data type of image is:\", img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV stores images in BGR format\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... but matplotlib stores images in RGB format\n",
    "plt.imshow(img[:, :, ::-1])\n",
    "\n",
    "# Alternatively\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[Split into BGR, HSV](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blue, green, red\n",
    "- Hue, saturation, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the image into the B,G,R components\n",
    "b, g, r = cv2.split(img)\n",
    "\n",
    "# Show the channels\n",
    "plt.figure(figsize=[20, 5])\n",
    "\n",
    "plt.subplot(141);plt.imshow(r, cmap=\"gray\");plt.title(\"Red Channel\")\n",
    "plt.subplot(142);plt.imshow(g, cmap=\"gray\");plt.title(\"Green Channel\")\n",
    "plt.subplot(143);plt.imshow(b, cmap=\"gray\");plt.title(\"Blue Channel\")\n",
    "\n",
    "# Merge the individual channels into a BGR image\n",
    "imgMerged = cv2.merge((b, g, r))\n",
    "# Show the merged output\n",
    "plt.subplot(144)\n",
    "plt.imshow(imgMerged[:, :, ::-1])\n",
    "plt.title(\"Merged Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Split the image into the B,G,R components\n",
    "h,s,v = cv2.split(img_hsv)\n",
    "\n",
    "# Show the channels\n",
    "plt.figure(figsize=[20,5])\n",
    "plt.subplot(141);plt.imshow(h, cmap=\"gray\");plt.title(\"H Channel\");\n",
    "plt.subplot(142);plt.imshow(s, cmap=\"gray\");plt.title(\"S Channel\");\n",
    "plt.subplot(143);plt.imshow(v, cmap=\"gray\");plt.title(\"V Channel\");\n",
    "plt.subplot(144);plt.imshow(img_rgb);   plt.title(\"Original\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Image Manipulations](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Cropping](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_region = img_rgb[200:400, 300:600]\n",
    "plt.imshow(cropped_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Resizing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[To a specific dimension](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing\n",
    "desired_width = 100\n",
    "desired_height = 200\n",
    "dim = (desired_width, desired_height)\n",
    "\n",
    "# Resize background image to sae size as logo image\n",
    "resized_cropped_region = cv2.resize(cropped_region, dsize=dim, interpolation=cv2.INTER_AREA)\n",
    "plt.imshow(resized_cropped_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[While maintaining aspect ratio](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using 'dsize'\n",
    "desired_width = 100\n",
    "aspect_ratio = desired_width / cropped_region.shape[1]\n",
    "desired_height = int(cropped_region.shape[0] * aspect_ratio)\n",
    "dim = (desired_width, desired_height)\n",
    "\n",
    "# Resize image\n",
    "resized_cropped_region = cv2.resize(cropped_region, dsize=dim, interpolation=cv2.INTER_AREA)\n",
    "plt.imshow(resized_cropped_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Flipping](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb_flipped_horz = cv2.flip(img_rgb, 1)\n",
    "img_rgb_flipped_vert = cv2.flip(img_rgb, 0)\n",
    "img_rgb_flipped_both = cv2.flip(img_rgb, -1)\n",
    "\n",
    "# Show the images\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(141);plt.imshow(img_rgb_flipped_horz);plt.title(\"Horizontal Flip\");\n",
    "plt.subplot(142);plt.imshow(img_rgb_flipped_vert);plt.title(\"Vertical Flip\");\n",
    "plt.subplot(143);plt.imshow(img_rgb_flipped_both);plt.title(\"Both Flipped\");\n",
    "plt.subplot(144);plt.imshow(img_rgb);plt.title(\"Original\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_4_'></a>[Brightness control - Addition](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.ones(img_rgb.shape, dtype=\"uint8\") * 50\n",
    "\n",
    "img_rgb_brighter = cv2.add(img_rgb, matrix)\n",
    "img_rgb_darker   = cv2.subtract(img_rgb, matrix)\n",
    "\n",
    "# Show the images\n",
    "plt.figure(figsize=[18, 5])\n",
    "plt.subplot(131); plt.imshow(img_rgb_darker);  plt.title(\"Darker\");\n",
    "plt.subplot(132); plt.imshow(img_rgb);         plt.title(\"Original\");\n",
    "plt.subplot(133); plt.imshow(img_rgb_brighter);plt.title(\"Brighter\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[Contrast control - multiplication](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial manipulation\n",
    "matrix1 = np.ones(img_rgb.shape) * 0.8\n",
    "matrix2 = np.ones(img_rgb.shape) * 1.2\n",
    "\n",
    "img_rgb_darker   = np.uint8(cv2.multiply(np.float64(img_rgb), matrix1))\n",
    "img_rgb_brighter = np.uint8(cv2.multiply(np.float64(img_rgb), matrix2))\n",
    "\n",
    "# Show the images\n",
    "plt.figure(figsize=[18,5])\n",
    "plt.subplot(131); plt.imshow(img_rgb_darker);  plt.title(\"Lower Contrast\");\n",
    "plt.subplot(132); plt.imshow(img_rgb);         plt.title(\"Original\");\n",
    "plt.subplot(133); plt.imshow(img_rgb_brighter);plt.title(\"Higher Contrast\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling overflow, i.e. pixels that are higher than 255\n",
    "matrix1 = np.ones(img_rgb.shape) * 0.8\n",
    "matrix2 = np.ones(img_rgb.shape) * 1.2\n",
    "\n",
    "img_rgb_lower  = np.uint8(cv2.multiply(np.float64(img_rgb), matrix1))\n",
    "img_rgb_higher = np.uint8(np.clip(cv2.multiply(np.float64(img_rgb), matrix2), 0, 255))\n",
    "\n",
    "# Show the images\n",
    "plt.figure(figsize=[18,5])\n",
    "plt.subplot(131); plt.imshow(img_rgb_lower); plt.title(\"Lower Contrast\");\n",
    "plt.subplot(132); plt.imshow(img_rgb);       plt.title(\"Original\");\n",
    "plt.subplot(133); plt.imshow(img_rgb_higher);plt.title(\"Higher Contrast\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_6_'></a>[Thresholding](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"img/ocr/building_windows/building_windows_1.jpg\"\n",
    "img_read = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "retval, img_thresh = cv2.threshold(img_read, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Show the images\n",
    "plt.figure(figsize=[18, 5])\n",
    "\n",
    "plt.subplot(121);plt.imshow(img_read, cmap=\"gray\");  plt.title(\"Original\")\n",
    "plt.subplot(122);plt.imshow(img_thresh, cmap=\"gray\");plt.title(\"Thresholded\")\n",
    "\n",
    "print(img_thresh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_6_1_'></a>[Application: OCR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original image\n",
    "img_path = \"img/ocr/handwritten_docs/handwritten_docs_1.jpg\"\n",
    "img_read = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Perform global thresholding\n",
    "retval, img_thresh_gbl_1 = cv2.threshold(img_read, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Perform global thresholding\n",
    "retval, img_thresh_gbl_2 = cv2.threshold(img_read, 130, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Perform adaptive thresholding\n",
    "img_thresh_adp = cv2.adaptiveThreshold(img_read, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, 7)\n",
    "\n",
    "# Show the images\n",
    "plt.figure(figsize=[18,15])\n",
    "plt.subplot(221); plt.imshow(img_read,        cmap=\"gray\");  plt.title(\"Original\");\n",
    "plt.subplot(222); plt.imshow(img_thresh_gbl_1,cmap=\"gray\");  plt.title(\"Thresholded (global: 50)\");\n",
    "plt.subplot(223); plt.imshow(img_thresh_gbl_2,cmap=\"gray\");  plt.title(\"Thresholded (global: 130)\");\n",
    "plt.subplot(224); plt.imshow(img_thresh_adp,  cmap=\"gray\");  plt.title(\"Thresholded (adaptive)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Demo - live image manipulation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access your camera, run the `camera.py` script in your terminal. To run the demo, use the `camera_demo.py` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[References](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code and examples were sourced from the [OpenCV Bootcamp](https://courses.opencv.org/courses/course-v1:OpenCV+Bootcamp+CV0/course/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Extra](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Pose estimation](https://courses.opencv.org/courses/course-v1:OpenCV+Bootcamp+CV0/courseware/457799bde2064b749df7fb0c0a741b5f/f9b682ac8b71470f963bc9c84ae3db39/1?activate_block_id=block-v1%3AOpenCV%2BBootcamp%2BCV0%2Btype%40vertical%2Bblock%400fb04585ef8e48819741446b1ec3bdbe)\n",
    "- [Image alignment](https://courses.opencv.org/courses/course-v1:OpenCV+Bootcamp+CV0/courseware/457799bde2064b749df7fb0c0a741b5f/e598b2c0c85f4d07bf509f6433c9efea/1?activate_block_id=block-v1%3AOpenCV%2BBootcamp%2BCV0%2Btype%40vertical%2Bblock%4062549c7193e94ac78c722367efe87270)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
