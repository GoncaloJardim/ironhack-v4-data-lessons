{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Unsupervised Learning](#toc1_)    \n",
    "  - [Load breast cancer data](#toc1_1_)    \n",
    "  - [Dimensionality reduction](#toc1_2_)    \n",
    "    - [PCA (Principal Component Analysis)](#toc1_2_1_)    \n",
    "    - [How to choose the number of principal components?](#toc1_2_2_)    \n",
    "  - [Clustering](#toc1_3_)    \n",
    "    - [Centroid-based clustering - K-Means](#toc1_3_1_)    \n",
    "      - [Elbow method - choosing the # of clusters](#toc1_3_1_1_)    \n",
    "    - [Agglomerative/Hierarchical/Connectivity-based clustering](#toc1_3_2_)    \n",
    "      - [Ward linkage](#toc1_3_2_1_)    \n",
    "      - [Complete linkage](#toc1_3_2_2_)    \n",
    "      - [Single linkage](#toc1_3_2_3_)    \n",
    "    - [Where is agglomerative clustering better than centroid-based clustering?](#toc1_3_3_)    \n",
    "    - [Density-based clustering - DBSCAN (Density-based spatial clustering of applications with noise)](#toc1_3_4_)    \n",
    "  - [Clustering metrics](#toc1_4_)    \n",
    "- [Resources](#toc2_)    \n",
    "- [References](#toc3_)    \n",
    "- [Acknowledgements](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Unsupervised Learning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwD_ZUaGN9b5"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = 'simple_white'\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Load breast cancer data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Learning Class\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer['data']\n",
    "y = cancer['target']\n",
    "display(X.shape)\n",
    "display(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize all the data in a dataframe\n",
    "data = pd.DataFrame(X, columns=cancer['feature_names'])\n",
    "data['label'] = y\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Dimensionality reduction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[PCA (Principal Component Analysis)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms the data into a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://imgs.search.brave.com/1u8pddu4FRUlSAuxj0i992lYN__bYby0JDUEqF2abEM/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9icmFk/bGV5Ym9laG1rZS5n/aXRodWIuaW8vSE9N/TC8xNS1wY2FfZmls/ZXMvZmlndXJlLWh0/bWwvY3JlYXRlLXBj/YS1pbWFnZS0xLnBu/Zw)  \n",
    "(Source: [Hands-on Machine Learning with R, Bradley Boehmke](https://bradleyboehmke.github.io/HOML/pca.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, we're not looking to predict whether or not a patient has a malignant/benign tumour, but if we can create 2 clusters corresponding to the most similar tumours. However, we think that the malignant tumours are significantly different from benign tumours, which is why we're looking to see if we can separate clusters based on the tumour type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check differences between features and labels\n",
    "plot_options, charts = plt.subplots(5, 6, figsize=(20, 7))\n",
    "malignant = data[data['label'] == 1]\n",
    "benign = data[data['label'] == 0]\n",
    "#ravel flattens the array so we don't need 2 indexes\n",
    "charts_1d = charts.ravel()\n",
    "for i in range(30):\n",
    "  charts_1d[i].hist(malignant.iloc[:, i], bins=50, alpha=.5)\n",
    "  charts_1d[i].hist(benign.iloc[:, i], bins=50, alpha=.5)\n",
    "  charts_1d[i].set_title(data.columns[i])\n",
    "  plot_options.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd want to see if we can cluster the tumours based on multiple features (like we did for the iris dataset some time ago) but in this case we have >30 labels, which makes a pairplot take ages to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(data, hue = 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still look to see if we can separate clusters based on 2 features at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster boundary for features 1, 29\n",
    "fig = px.scatter(x=data.iloc[:, 1], y=data.iloc[:, 29], color=y, opacity=0.5)\n",
    "fig.update_layout(xaxis_title=data.columns[1], yaxis_title=data.columns[29], height=600, width=600)\n",
    "fig.show()\n",
    "\n",
    "# Cluster boundary for features 28, 29\n",
    "fig = px.scatter(x=data.iloc[:, 28], y=data.iloc[:, 29], color=y, opacity=0.5)\n",
    "fig.update_layout(xaxis_title=data.columns[28], yaxis_title=data.columns[29], height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(cancer['data'])\n",
    "\n",
    "X_scaled = scaler.transform(cancer['data'])\n",
    "pd.DataFrame(X_scaled, columns=cancer.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need to perform scaling before running the PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the PCA object\n",
    "# the number of components chosen will be the new number of features!\n",
    "pca = PCA(n_components=3)\n",
    "# fit the PCA model to breast cancer data\n",
    "pca.fit(X_scaled)\n",
    "# it's like we have three new axis (those defined by the PCA principal components)\n",
    "X_pca = pca.transform(X_scaled)\n",
    "pd.DataFrame(X_pca, columns=['PCA_1', 'PCA_2', 'PCA_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X.shape)\n",
    "display(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_pca[:, 0], y=X_pca[:, 1], color=y, opacity=0.5)\n",
    "fig.update_layout(xaxis_title='1st principal component', yaxis_title='2nd principal component', height=600, width=600)\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(x=X_pca[:, 1], y=X_pca[:, 2], color=y, opacity=0.5)\n",
    "fig.update_layout(xaxis_title='2nd principal component', yaxis_title='3rd principal component', height=600, width=600)\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(x=X_pca[:, 0], y=X_pca[:, 2], color=y, opacity=0.5)\n",
    "fig.update_layout(xaxis_title='1st principal component', yaxis_title='3rd principal component', height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[How to choose the number of principal components?](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each PCA feature has a little bit of each original feture\n",
    "# The PCA tells you how much of the original features each new feature contains\n",
    "pd.DataFrame(pca.components_, columns=cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib\n",
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1, 2], [\"First component\", \"Second component\",\"Third component\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(data.columns)), data.columns, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mathematically, the new features (principal components) are a LINEAR COMBINATION of the previous (old) features and the weights of each of them are represented in the diagram above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Clustering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[Centroid-based clustering - K-Means](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.sanity.io/images/kuana2sp/production/4a7a2b92082d482c56e0c6396064ca23074168ac-1020x752.png?w=1080&fit=max&auto=format)  \n",
    "(Source: [Getting started with k-means clustering in Python, Dr J Rogel-Salazar](https://domino.ai/blog/getting-started-with-k-means-clustering-in-python))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "p6nIROyG6n9L",
    "outputId": "0728c9a8-2684-404f-941e-824232c867a5"
   },
   "outputs": [],
   "source": [
    "# Creating 3 clusters artificially to illustrate the algorithm\n",
    "n_samples = 1500\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, centers=3, cluster_std=0.7, n_features=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review initial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run clustering with:\n",
    "\n",
    "a) k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "K9Dq6mveD5NA",
    "outputId": "cf438724-f850-480e-8ff7-184faadcaf22"
   },
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "pred = kmeans.predict(X)\n",
    "\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "StVvxxOhDghk",
    "outputId": "dbcbe303-68fc-484a-fcc1-3e48a19e0926"
   },
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "pred = kmeans.predict(X)\n",
    "\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "1s6LnkHE6n_9",
    "outputId": "5beb7a03-e208-458f-91e5-78eecf507d8f"
   },
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "pred = kmeans.predict(X)\n",
    "\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_1_1_'></a>[Elbow method - choosing the # of clusters](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a number of clusters using the elbow heuristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cluster.KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2,20))\n",
    "visualizer.fit(X)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, any k between 4 and 8 would be good enough for this problem. Remember, we're not looking to minimize the error in this case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cluster.KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1495,1499))\n",
    "visualizer.fit(X)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to minimize the error, we end up with an unusable number of clusters, which defeats the purpose of running the algorithm to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[Agglomerative/Hierarchical/Connectivity-based clustering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20200204181551/Untitled-Diagram71.png)  \n",
    "(Source: [Hierarchical Clustering in Machine Learning, Geeks 4 Geeks](https://www.geeksforgeeks.org/ml-hierarchical-clustering-agglomerative-and-divisive-clustering/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_2_1_'></a>[Ward linkage](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "d0MkbX9PRHKj",
    "outputId": "f7f0aeaa-8d95-472a-92a9-77a219c27779"
   },
   "outputs": [],
   "source": [
    "# ward linkage tends to produce relatively equally sized clusters\n",
    "agglomerative = cluster.AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "pred = agglomerative.fit_predict(X)\n",
    "\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_2_2_'></a>[Complete linkage](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "QznfD0ONRmC6",
    "outputId": "5189539b-ec7d-4f42-e059-4a45ad94be1c"
   },
   "outputs": [],
   "source": [
    "# complete linkage penalizes heavily outliers\n",
    "agglomerative = cluster.AgglomerativeClustering(n_clusters=3,linkage='complete')\n",
    "pred = agglomerative.fit_predict(X)\n",
    "\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_3_2_3_'></a>[Single linkage](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "l2X6SSTfSWEU",
    "outputId": "011b97e2-ee73-4361-dee0-48e2cc7da666"
   },
   "outputs": [],
   "source": [
    "# different algorithms are good for different applications\n",
    "agglomerative = cluster.AgglomerativeClustering(n_clusters=3, linkage='single')\n",
    "pred = agglomerative.fit_predict(X)\n",
    "\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_3_'></a>[Where is agglomerative clustering better than centroid-based clustering?](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "zzrjXjRGl-rj",
    "outputId": "34d61f96-e3e7-46a0-b5b0-e87f6939aefc"
   },
   "outputs": [],
   "source": [
    "# Create moon-like clusters instead\n",
    "n_samples = 1500\n",
    "X, y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Run K-Means on non-radial clusters\n",
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "y1_pred = kmeans.predict(X)\n",
    "\n",
    "# Run Agglomerative clustering on non-radial clusters\n",
    "single = cluster.AgglomerativeClustering(n_clusters=2, linkage='single')\n",
    "y2_pred = single.fit_predict(X)\n",
    "\n",
    "# Review results\n",
    "options, charts = plt.subplots(1, 2, figsize=(12, 6))\n",
    "colors = np.array(['blue', 'red'])\n",
    "charts[0].scatter(X[:, 0], X[:, 1], color=colors[y1_pred])\n",
    "charts[1].scatter(X[:, 0], X[:, 1], color=colors[y2_pred])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_4_'></a>[Density-based clustering - DBSCAN (Density-based spatial clustering of applications with noise)](#toc0_)\n",
    "\n",
    "> In density-based clustering, clusters are defined as areas of higher density than the remainder of the data set. Objects in sparse areas – that are required to separate clusters – are usually considered to be noise and border points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "X, y = datasets.make_moons(n_samples=n_samples, noise=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = cluster.DBSCAN(eps=0.1, min_samples=5) #change maximum distance and see efect\n",
    "# eps is the maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "# min_samples is the number of samples in a neighborhood for a point to be considered as a core point.\n",
    "\n",
    "pred = dbs.fit_predict(X)\n",
    "print(pred)\n",
    "set(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "colors = np.array(['blue', 'red', 'black', 'green', 'yellow'])\n",
    "plt.scatter(X[:, 0], X[:, 1], color=colors[pred])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Clustering metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Silhouette Score** [$^{[3]}$](https://www.educative.io/answers/what-is-silhouette-score)\n",
    "\n",
    "> Silhouette Score is a tool for assessing the appropriateness of clustering results by providing a quantitative measure of how well-defined and distinct the clusters are. The Silhouette Score quantifies **how well a data point fits into its assigned cluster** and **how distinct it is from other clusters** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Score = $\\frac{(b - a)}{max(a, b)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "LKLMgR4bgpMy",
    "outputId": "3701dc2a-aef9-49a0-fe88-c4091d2ea4e3"
   },
   "outputs": [],
   "source": [
    "# Re-create clusters\n",
    "n_samples = 1500\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, centers=3, cluster_std=0.7, n_features=2, random_state=0)\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "pred = kmeans.predict(X)\n",
    "\n",
    "print(\"Model 1 Silhouette Score: {}\".format(silhouette_score(X, pred, metric='euclidean')))\n",
    "\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "AXAoqsPjhHhC",
    "outputId": "a84a1c94-191c-4489-c6b6-32a18308900d"
   },
   "outputs": [],
   "source": [
    "agglomerative = cluster.AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
    "pred = agglomerative.fit_predict(X)\n",
    "\n",
    "print(\"Model 2 Silhouette Score: {}\".format(silhouette_score(X, pred, metric='euclidean')))\n",
    "\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "TmebPqlAhM83",
    "outputId": "4abd74ec-8e2b-409e-f670-be412fc059b2"
   },
   "outputs": [],
   "source": [
    "agglomerative = cluster.AgglomerativeClustering(n_clusters=3, linkage='single')\n",
    "pred = agglomerative.fit_predict(X)\n",
    "\n",
    "print(\"Model 3 Silhouette Score: {}\".format(silhouette_score(X, pred, metric='euclidean')))\n",
    "\n",
    "fig = px.scatter(x=X[:, 0], y=X[:, 1], color=pred, opacity=0.5)\n",
    "fig.update_layout(height=600, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Silhouette Score ranges from -1 to +1. Here is how to interpret the value: [$^{[3]}$](https://www.educative.io/answers/what-is-silhouette-score)\n",
    "\n",
    "**Negative**\n",
    "> A negative score indicates that the **data point is likely assigned to the wrong cluster**, as its distance to its assigned cluster’s points is greater than its distance to the nearest neighboring cluster’s points.\n",
    "\n",
    "**Close to 0**\n",
    "> A score close to 0 implies that the **data point is on or very close to the decision boundary between two clusters**. It indicates that the clustering is not well-defined and can be ambiguous.\n",
    "\n",
    "**Positive**\n",
    "> A positive score indicates that the **data point is appropriately clustered**, and **its distance to its assigned cluster’s points is smaller than its distance to the nearest neighboring cluster’s points**. A score close to +1 suggests that the data point is well-clustered and distinctly separated from other clusters. It is a strong indication of a meaningful clustering result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the silhouette score fail to be a good measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "X, y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "y1_pred = kmeans.predict(X)\n",
    "\n",
    "\n",
    "single = cluster.AgglomerativeClustering(n_clusters=2, linkage='single')\n",
    "y2_pred = single.fit_predict(X)\n",
    "\n",
    "\n",
    "options, charts = plt.subplots(1, 2, figsize=(12, 6))\n",
    "colors = np.array(['blue', 'red'])\n",
    "charts[0].scatter(X[:, 0], X[:, 1], color=colors[y1_pred])\n",
    "charts[1].scatter(X[:, 0], X[:, 1], color=colors[y2_pred])\n",
    "plt.show()\n",
    "\n",
    "print(\"Model 1 Silhouette Score: {}\".format(silhouette_score(X, y1_pred)))\n",
    "print(\"Model 2 Silhouette Score: {}\".format(silhouette_score(X, y2_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Resources](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA by StatQuest:\n",
    "- [Main ideas (5 min)](https://www.youtube.com/watch?v=HMOI_lkzW08)\n",
    "- [Step-by-step (22 min)](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n",
    "- [Practical Tips (8 min)](https://www.youtube.com/watch?v=oRvgq966yZg)  \n",
    "\n",
    "[t-SNEs by StatQuest (12 min)](https://www.youtube.com/watch?v=NEaUSP4YerM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[References](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [What is Silhouette Score, Educative IO](https://www.educative.io/answers/what-is-silhouette-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Acknowledgements](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you, David Henriques, for your awesome lesson structure and content!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lizzy_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
