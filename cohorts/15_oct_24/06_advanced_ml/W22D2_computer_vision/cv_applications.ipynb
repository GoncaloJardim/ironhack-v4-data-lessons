{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Computer Vision Applications](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Computer Vision Applications](#toc1_)    \n",
    "  - [Image classification](#toc1_1_)    \n",
    "    - [Building a Digit Classifier from Scratch](#toc1_1_1_)    \n",
    "    - [Using a Pre-trained Model for Advanced Classification](#toc1_1_2_)    \n",
    "  - [Object Detection](#toc1_2_)    \n",
    "    - [Object Detection with Faster R-CNN](#toc1_2_1_)    \n",
    "  - [Image Segmentation](#toc1_3_)    \n",
    "    - [Image Segmentation with Detectron2](#toc1_3_1_)    \n",
    "- [Tips](#toc2_)    \n",
    "  - [Why use PyTorch instead of NumPy to define tensors?](#toc2_1_)    \n",
    "- [Extra](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a while to run...\n",
    "# !pip install torch torchvision albumentations opencv-python\n",
    "# !python -m pip install \"git+https://github.com/facebookresearch/detectron2.git\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import filters\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if you have a GPU available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available()) # If False, you don't have a GPU available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is for Google Colab, where you can get a GPU for faster computation. You likely don't have a dedicated GPU on your computer, unless you're a gamer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library (`nvcc`) is specifically for an NVIDIA GPU so the output message would look like this if you had one:\n",
    "\n",
    "```\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2020 NVIDIA Corporation\n",
    "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
    "Cuda compilation tools, release 11.1, V11.1.105\n",
    "Build cuda_11.1.TC455_06.29190527_0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the difference between a CPU and a GPU, here's a [video for you](https://www.youtube.com/watch?v=-P28LKWTzrI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    return transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Image classification](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image classification is the task of assigning a label to an image from a predefined set of categories.\n",
    "\n",
    "### <a id='toc1_1_1_'></a>[Building a Digit Classifier from Scratch](#toc0_)\n",
    "\n",
    "Let's build a simple Convolutional Neural Network (CNN) to classify handwritten digits using the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# Display some examples\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Label: {y_train[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=128, \n",
    "                    epochs=5, \n",
    "                    validation_split=0.1,\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]}')\n",
    "print(f'Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and visualize results\n",
    "predictions = model.predict(X_test[:10])\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test[:10], axis=1)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'True: {true_classes[i]}, Pred: {predicted_classes[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Using a Pre-trained Model for Advanced Classification](#toc0_)\n",
    "\n",
    "Now, let's use a pre-trained model for a more complex classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50 model\n",
    "model = tf.keras.applications.ResNet50(weights='imagenet')\n",
    "\n",
    "# Function to preprocess and predict\n",
    "def predict_image(image_path):\n",
    "    # Load and preprocess image\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    decoded_predictions = tf.keras.applications.resnet50.decode_predictions(predictions, top=5)[0]\n",
    "    \n",
    "    # Display image and predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    y_pos = np.arange(5)\n",
    "    plt.barh(y_pos, [pred[2] for pred in decoded_predictions])\n",
    "    plt.yticks(y_pos, [pred[1] for pred in decoded_predictions])\n",
    "    plt.xlabel('Probability')\n",
    "    plt.title('Top 5 Predictions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (replace with your own image path)\n",
    "predictions = predict_image('../../../../img/computer-vision/classification/cats/cat_2.jpg')\n",
    "print(\"Top predictions:\")\n",
    "for i, (imagenet_id, label, score) in enumerate(predictions):\n",
    "    print(f\"{i+1}: {label} ({score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50 is a deep CNN that's been pre-trained on millions of images. It can recognize thousands of different object categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Object Detection](#toc0_)\n",
    "Object detection is a computer vision technique that identifies and localizes objects within an image. Unlike image classification, which assigns a single label to an image, object detection provides bounding boxes around detected objects. Common object detection models include YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), and Faster R-CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Object Detection with Faster R-CNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(image_path):\n",
    "    \"\"\"Performs object detection on an image using Faster R-CNN.\"\"\"\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    \n",
    "    image_tensor = load_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)\n",
    "    \n",
    "    # Display the image with bounding boxes\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    for i, box in enumerate(prediction[0]['boxes']):\n",
    "        x1, y1, x2, y2 = map(int, box.numpy())\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection(\"../../../../img/computer-vision/object-detection/image_1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Image Segmentation](#toc0_)\n",
    "Image segmentation is the process of partitioning an image into multiple regions to identify objects more precisely at the pixel level. There are two main types:\n",
    "- **Semantic Segmentation**: Classifies each pixel in an image into a category (e.g., sky, car, road).\n",
    "- **Instance Segmentation**: Distinguishes between individual objects of the same class (e.g., two different cars in an image). Models like U-Net and Mask R-CNN are commonly used for segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[Image Segmentation with Detectron2](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Detectron2 is Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of Detectron and maskrcnn-benchmark. It supports a number of computer vision research projects and production applications in Facebook. *(Source: [Facebook GitHub](https://github.com/facebookresearch/detectron2))*\n",
    "\n",
    "![](../../../../img/detectron2.png)  \n",
    "(Source: [Facebook GitHub](https://github.com/facebookresearch/detectron2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_segmentation(image_path):\n",
    "    \"\"\"Performs instance segmentation on an image using Detectron2's Mask R-CNN.\"\"\"\n",
    "    cfg = get_cfg()\n",
    "    cfg.MODEL.DEVICE = \"cpu\" # Comment this out if you have a GPU\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    outputs = predictor(image)\n",
    "    \n",
    "    # Display segmented mask\n",
    "    v = outputs['instances'].pred_masks.cpu().numpy()\n",
    "    plt.imshow(v[0], cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_segmentation(\"../../../../img/computer-vision/object-detection/image_1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Tips](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using typical architectures is the industry standard -> to figure out your own, academia is probably a better place for you\n",
    "* Pre-trained models are your best bet at a useful models -> will save you from general edge, color, and feature extraction\n",
    "* Whenever you see CUDA, it's just the equivalent of C (low-level programming language, known for speed) on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Why use PyTorch instead of NumPy to define tensors?](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [NumPy](https://numpy.org/) is the most widely used library for scientific and numeric programming in Python. It provides very similar functionality and a very similar API to that provided by PyTorch; however, **it does not support using the GPU or calculating gradients, which are both critical for deep learning**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Extra](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"Visualizing and Understanding Convolutional Networks\"](https://arxiv.org/pdf/1311.2901.pdf)\n",
    "Get examples from Practical DL for coders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
